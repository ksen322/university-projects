# Аппроксимация функций на многослойной сети прямого распространения  
## Теоретические сведения
МНСПР может осуществлять преобразование произвольной сложности входного вектора $X={x_{1},x_{2},…,x_{n}}$ в выходной $Y={y_{1},y_{2},…,y_{n}}$.   
Она состоит из простых вычислительных элементов (нейронов), которые связаны между собой посредством синаптических связей. Ее особенностью является послойная организация нейронов и односторонняя передача сигнала от предыдущего слоя к последующему.  
Слои нейронов с первого по предпоследний называются скрытыми слоями, а последний слой называется выходным.  

![image](https://github.com/ksen322/university-projects/assets/119673458/71f214fe-939e-450b-b2d9-193ea5506a77)

Нейрон представляется, как последовательное соединение сумматора и функции преобразования $f_{j}^{k(n)}$. На его вход поступают сигналы с выходов всех нейронов предыдущего (k–1) слоя $a_{i}^{(k-1)} (i=1,…,N_(k-1))$ , умноженные на соответствующий весовой коэффициент связи $w_{ij}^k$ (синапс).  
Основными функциями активации в нейроне являются следующие:  

![image](https://github.com/ksen322/university-projects/assets/119673458/63337605-9818-4611-8e0d-f651df5c60d7)

## Ход выполнения   
Для создания собственной нейронной сети было решено использовать программу разработчика PyCharm, а также подключить дополнительные библиотеки torch (для удобных инструментов создания нейронных сетей) и matplotlib (для вывода результата работы нейронной сети в виде графиков).  

Делаем аппроксимацию sin⁡(x + π/2)  

_Как выглядит функция sin(x + π/2)_

![image](https://github.com/ksen322/university-projects/assets/119673458/8fa0e199-a103-4215-8f29-dbe57a3d595f)

Создание нейронной сети делиться на 4 этапа:  
1)	Создание данных (тренировочная и тестовые выборки)
2)	Создание модели МНСПР
3)	Обучение нейронной сети
4)	Тестирование нейронной сети

### Создание данных
Первым шагом создаются 2 выборки данных: тренировочная и тестовая. На тренировочной выборке значения x случайно генерируются в диапазоне [0; 1], а y значения прогоняются через функцию sin(x + pi/2).  
Таким образом тренировочная функция представляет собой sin(x + pi/2), но с многократными пробелами.

Первоначальная сгенерированная выборка

![image](https://github.com/ksen322/university-projects/assets/119673458/ac217e9a-10bb-40da-9035-984c229ad084)

Получившаяся выборка слишком очевидна для того, чтобы быть тренировочными данными, поэтому исказим ее, использовав шум.  

![image](https://github.com/ksen322/university-projects/assets/119673458/7fa70a3c-fb75-418e-8418-350943734784)  

Создание тестовой выборки отличаются лишь тем, что значения x создаются не случайно, а равномерно по всей функции. Таким образом график получается легко узнаваемым для человека, но не для нейронной сети, в графике по-прежнему много пробелов.  

![image](https://github.com/ksen322/university-projects/assets/119673458/db484085-e95c-4f17-981b-99329b3e3905)

Данные описываются классом tensor из библиотеки torch. Чтобы будущая модель смогла принять эти данные, применяется метод unsqueeze.  

### Создание модели МНСПР  
Используется стандартная модель перспетрона со следующими параметрами:  
<details><summary>Используется стандартная модель перспетрона со следующими параметрами: </summary>
  
1)	Кол-во нейронов входного слоя – 1
2)	Кол-во скрытых слоев – 1
3)	Кол-во нейронов в каждом скрытом слое – 300
4)	Кол-во нейронов выходного слоя – 1
5)	Обсчитывающее устройство – ЦПУ

</details>

А также функция, которая позволяет прогонять тренировочные выборки через скрытие слои.  
### Обучение нейронной сети
Весь процесс обучения нейронной сети представляет собой поиск антиградиента для того, чтобы получить локальный минимум функции ошибок. В локальном минимуме точность нашей нейронной сети максимальна. Для того чтобы сохранять прогресс обучения и продолжать двигаться в сторону локального максимума во время обучения изменяются весовые коэффициенты.  

Если прогнать данные через модель нейронной сети без обучения, то мы получим инициализацию данных, от которых начнет обучаться наша нейронная сеть.  

![image](https://github.com/ksen322/university-projects/assets/119673458/9cbcbf7b-9056-4a64-a726-0df271b0b916)  

Далее на протяжении 5000 эпох попробуем обучить нейронную сеть предсказывать функцию sin⁡(x + π/2).  

![image](https://github.com/ksen322/university-projects/assets/119673458/7803d68a-22d2-4dc8-aec4-fdac4265274d)

### Тестирование
Для того, чтобы улучшить параметры нейронной сети, следует посмотреть на ее текущую точность. Для этого используется loss функция.  
```
tensor(0.9049, grad_fn=<MeanBackward0>)
tensor(0.2415, grad_fn=<MeanBackward0>)
tensor(0.0318, grad_fn=<MeanBackward0>)
tensor(0.0088, grad_fn=<MeanBackward0>)
tensor(0.0048, grad_fn=<MeanBackward0>)
```

Точность нашей нейронной сети около 99,5%, что считается достаточно хорошим показателем. Поэтому для текущего количества данных, параметры оставляем теми же.  

#### Тест 1. Подадим укороченную тест выборку
Точность 98%  

![image](https://github.com/ksen322/university-projects/assets/119673458/03f1030c-ff8f-4ea1-a2d7-270522d25e51)

#### Тест 2. Подадим увеличенную тест выборку
Нейронная сеть успешно предсказывала функцию до граничного значения, на котором она обучалась. Чтобы сеть смогла справиться с большим количеством данных, нужно больше данных и времени на обучение.  

![image](https://github.com/ksen322/university-projects/assets/119673458/86b5cb89-4e85-45dc-867b-f7861fd09fe7)

#### Тест 3. Уменьшим количество нейронов с 300 до 3
Нейронная сеть не успевает доучиться, то есть не успевает принять большое количество данных.  

![image](https://github.com/ksen322/university-projects/assets/119673458/ff32a541-cb2b-44ca-afbc-385226f2c17a)

#### Тест 4. Уменьшим количество нейронов с 300 до 30
Точность достигла 99,7%, что говорит об очень хорошем результате, а скорость обучения увеличилась в 10 раз.

![image](https://github.com/ksen322/university-projects/assets/119673458/4702a0c3-67b4-4c01-9520-10712c29ec79)
